# -*- coding: utf-8 -*-
"""CT-predict-pretrain.ipynb のコピー

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hYPGM9qOusQaXOUXnsT1hmFKCsQb05I9
"""

from math import ceil
import torch
from torch import nn

## スクラッチでEfficientnetを作っていく。

class Swish(nn.Module):
  def forward(self, x):
    return x * torch.sigmoid(x)


class SEblock(nn.Module):
  def __init__(self, in_channels, channel_squeeze):
    super().__init__()
    self.se = nn.Sequential(
        nn.AdaptiveAvgPool2d(1),
        nn.Conv2d(in_channels, channel_squeeze, 1),
        Swish(),
        nn.Conv2d(channel_squeeze, in_channels, 1),
    )
    self.se.apply(weight_init)

    def forward(self, x):
      return x * (torch.sigmoid(self.se(x)))


def weight_init(m):
  if isinstance(m, nn.Conv2d):
    nn.init.kaiming_normal_(m.weight)

  if isinstance(m, nn.Linear):
    nn.init.kaiming_uniform_(m.weight)
    nn.init.zeros_(m.bias)


class ConvBN(nn.Module):
  def __init__(self, in_channels, out_channels,
               kernel_size, stride=1, padding=0, groups=1):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups, bias=False),
        nn.BatchNorm2d(out_channels),
    )
    self.layers.apply(weight_init)

  def forward(self, x):
    return self.layers(x)


class DropConnect(nn.Module):
  def __init__(self, drop_rate):
    super().__init__()
    self.drop_rate = drop_rate
  
  def forward(self, x):
    if self.training:
      keep_rate = 1.0 - self.drop_rate
      r = torch.rand([x.size(0),1,1,1], dtype=x.dtype).to(x.device)
      r += keep_rate
      mask = r.floor()
      return x.div(keep_rate) * mask
    else:
      return x


class MBConvBlock(nn.Module):
  def __init__(self, in_channels, out_channels, expand_ratio, stride,
               kernel_size, reduction_ratio=4, drop_connect_rate=0.2):
    super().__init__()
    self.use_residual = (in_channels==out_channels) & (stride==1)
    ch_med = int(in_channels * expand_ratio)
    ch_sq = max(1, in_channels // reduction_ratio)

    if expand_ratio != 1.0:
      layers = [ConvBN(in_channels, ch_med, 1), 
                Swish()]
    else:
      layers = []

    layers.extend([
                   ConvBN(ch_med, ch_med, kernel_size, stride=stride,
                          padding=(kernel_size-1)//2, groups=ch_med),
                   Swish(),
                   SEblock(ch_med, ch_sq),
                   ConvBN(ch_med, out_channels, 1),
    ])
    if self.use_residual:
      self.drop_connect = DropConnect(drop_connect_rate)

    self.layers = nn.Sequential(*layers)

  def forward(self, x):
    if self.use_residual:
      return x + self.drop_connect(self.layers(x))
    else:
      return self.layers(x)


class Flatten(nn.Module):
  def forward(self, x):
    return x.view(x.shape[0], -1)


class _EfficientNet(nn.Module):
  def __init__(self, width_mult=1.0, depth_mult=1.0,
               resolution=False, dropout_rate=0.2,
               in_channels=3, num_classes=1000):
    super().__init__()

    # expand_ration, channel, repeats, stride, kernel_size
    base_models = [
                  [1, 16, 1, 1, 3],
                  [6, 24, 2, 2, 3],
                  [6, 40, 2, 2, 5],
                  [6, 80, 3, 2, 3],
                  [6, 112, 3, 1, 5],
                  [6, 192, 4, 2, 5],
                  [6, 320, 1, 1, 3],
    ]

    out_channels = int(math.ceil(32*width_mult))
    features = [nn.AdaptiveAvgPool2d(resolution)] if resolution else []
    features.extend([ConvBN(in_channels, out_channels, 3, stride=2), Swish()])

    in_channels = out_channels
    for t, c, n, s, k in base_models:
      out_channels = int(math.ceil(c*width_mult))
      repeats = int(math.ceil(n*depth_mult))
      for i in range(repeats):
        stride = s if i==0 else 1
        features.extend([MBConvBlock(in_channels, out_channels, t, stride, k)])
        in_channles = out_channels

    last_channel = int(math.ceil(1280*width_mult))
    features.extend([ConvBN(in_channels, last_channel, 1), Swish()])

    self.features = nn.Sequential(*features)
    self.classifier = nn.Sequential(
        nn.AdaptiveAvgPool2d(1),
        Flatten(),
        nn.Dropout(dropout_rate),
        nn.Linear(last_channel, num_classes),
    )

  def forward(self, x):
    x = self.features(x)
    x = self.classifier(x)
    return x

base_model = [
    # expand_ratio, channels, repeats, stride, kernel_size
    [1, 16, 1, 1, 3],
    [6, 24, 2, 2, 3],
    [6, 40, 2, 2, 5],
    [6, 80, 3, 2, 3],
    [6, 112, 3, 1, 5],
    [6, 192, 4, 2, 5],
    [6, 320, 1, 1, 3],
]

phi_values = {
    # (phi_value, resolution, drop_rate)
    "b0": (0, 224, 0.2),  
    "b1": (0.5, 240, 0.2),
    "b2": (1, 260, 0.3),
    "b3": (2, 300, 0.3),
    "b4": (3, 380, 0.4),
    "b5": (4, 456, 0.4),
    "b6": (5, 528, 0.5),
    "b7": (6, 600, 0.5),
}

class CNNBlock(nn.Module):
  def __init__(self, in_channels, out_channels,
               kernel_size, stride, padding, groups=1):
    super(CNNBlock, self).__init__()
    # もしgoups=1なら通常のconv, goups=inchannelsならdepthwiseconvになる。
    self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)
    self.bn = nn.BatchNorm2d(out_channels)
    self.silu = nn.SiLU() # SiLUとSwish同じ。

  def forward(self, x):
    return self.silu(self.bn(self.cnn(x)))


class SqueezeExcitation(nn.Module):
  def __init__(self, in_channels, reduced_dim):
    super(SqueezeExcitation, self).__init__()
    self.se = nn.Sequential(
        nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1
        nn.Conv2d(in_channels, reduced_dim, 1),
        nn.SiLU(),
        nn.Conv2d(reduced_dim, in_channels, 1),
        nn.Sigmoid(),
    )

  def forward(self, x):
    return x * self.se(x)


class InvertedResidualBlock(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size,
               stride, padding, expand_ratio, reduction=4, survival_prob=0.8):
    super(InvertedResidualBlock, self).__init__()
    self.survival_prob = 0.8
    self.use_residual = in_channels == out_channels and stride == 1 # 論文で言う繰り返されるstageで、2回目からTrueになる。
    hidden_dim = in_channels * expand_ratio
    self.expand = in_channels != hidden_dim
    reduced_dim = int(in_channels / reduction)

    if self.expand:
      self.expand_conv = CNNBlock(
          in_channels, hidden_dim, kernel_size=3, stride=1, padding=1
      )
    self.conv = nn.Sequential(
        CNNBlock(
            hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,
        ),
        SqueezeExcitation(hidden_dim, reduced_dim),
        nn.Conv2d(hidden_dim, out_channels, 1, bias=False),
        nn.BatchNorm2d(out_channels),
    )

  def stochastic_depth(self, x):
    if not self.training:
      return x

    binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob
    return torch.div(x, self.survival_prob) * binary_tensor

  def forward(self, inputs):
    x = self.expand_conv(inputs) if self.expand else inputs

    if self.use_residual:
      return self.stochastic_depth(self.conv(x)) + inputs
    else:
      return self.conv(x)


class EfficientNet(nn.Module):
  def __init__(self, version, num_classes):
    super(EfficientNet, self).__init__()
    width_factor, depth_factor, dropout_rate = self.calculate_factors(version)
    last_channels = ceil(1280 * width_factor) # ceilは与えた数以上の最小の整数を返す. ceil(.95) -> 1
    self.pool = nn.AdaptiveAvgPool2d(1)
    self.features = self.create_features(width_factor, depth_factor, last_channels)
    self.classifier = nn.Sequential(
        nn.Dropout(dropout_rate),
        nn.Linear(last_channels, num_classes),
    )

  def calculate_factors(self, version, alpha=1.2, beta=1.1):
    phi, res, drop_rate = phi_values[version]
    depth_factor = alpha ** phi
    width_factor = beta ** phi
    return width_factor, depth_factor, drop_rate

  def create_features(self, width_factor, depth_factor, last_channels):
    channels = int(32 * width_factor)
    features = [CNNBlock(3, channels, 3, stride=2, padding=1)]
    in_channels = channels

    for expand_ratio, channels, repeats, stride, kernel_size in base_model:
      out_channels = 4*ceil(int(channels*width_factor) / 4)
      layers_repeats = ceil(repeats * depth_factor)

      for layer in range(layers_repeats):
        features.append(
            InvertedResidualBlock(
                in_channels,
                out_channels,
                expand_ratio=expand_ratio,
                stride = stride if layer == 0 else 1, # layer0以外では、H,Wが変わらないようにしている。
                kernel_size=kernel_size,
                padding=kernel_size//2, # H,Wが変わらないようにしている。
            )
        )
        in_channels = out_channels
    features.append(
        CNNBlock(in_channels, last_channels, kernel_size=1, stride=1, padding=0)
    )

    return nn.Sequential(*features)

  def forward(self, x):
    x = self.pool(self.features(x))
    return self.classifier(x.view(x.shape[0], -1))


def test():
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  version = 'b0'
  phi, res, drop_rate = phi_values[version]
  num_examples, num_classes = 4, 10
  x = torch.randn((num_examples, 3, res, res)).to(device)
  model = EfficientNet(
      version=version,
      num_classes = num_classes,
  ).to(device)
  
  result = model(x)
  print(result.shape) # (num_examples, num_classes)

test()

# def _make_divisible(v, divisor, min_value=None):
#   '''
#   この関数は、元のtfリポジトリから取得されます。
#     これにより、すべてのレイヤーに8で割り切れるチャネル番号が確実に割り当てられます。
#     それはここで見ることができます：
#     https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
#     ：param v：
#     ：param除数：
#     ：param min_value：
#     ：戻る：
#   '''
#   if min_value is None:
#     min_value = divisor
#   new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
#   if new_v < 0.9 * v:
#     new_v += divisor
#   return new_v


# if hasattr(nn, 'SiLU'):
#   SiLU = nn.SiLU
# else:
#   # 古いバージョンとの互換性のため。
#   class SiLU(nn.Module):
#     def forward(self, x):
#       return x * torch.sigmoid(x)


# class SELayer(nn.Module):
#   def __init__(self, inp, oup, reduction=4):
#     super(SELayer, self). __init__()
#     self.avg_pool = nn.AdaptiveAvgPool2d(1)
#     self.fc = nn.Sequential(
#           nn.Linear(oup, _make_divisible(inp // reduction, 8)),
#           SiLU(),
#           nn.Linear(_make_divisible(inp // reduction, 8), oup),
#           nn.Sigmoid()
#     )

#   def forward(self, x):
#     # print(x.size())
#     b, c, _, _ = x.size()
#     y = self.avg_pool(x).view(b, c)
#     y = self.fc(y).view(b, c, 1, 1)
#     return x * y


# def conv_3x3_bn(inp, oup, stride):
#   return nn.Sequential(
#       nn.Conv2d(inp, oup, 3, stride, 1, bias=False),
#       nn.BatchNorm2d(oup),
#       SiLU()
#   )

# def conv_1x1_bn(inp, oup):
#   return nn.Sequential(
#       nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
#       nn.BatchNorm2d(oup),
#       SiLU()
#   )


# class MBConv(nn.Module):
#   def __init__(self, inp, oup, stride, expand_ratio, use_se):
#     super(MBConv, self).__init__()
#     assert stride in [1, 2]

#     hidden_dim = round(inp * expand_ratio)
#     self.identity = stride == 1 and inp == oup
#     if use_se:
#       self.conv = nn.Sequential(
#           # pw
#           nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),
#           nn.BatchNorm2d(hidden_dim),
#           SiLU(),
#           # dw
#           nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
#           nn.BatchNorm2d(hidden_dim),
#           SiLU(),
#           SELayer(inp, hidden_dim),
#           # pw-linear
#           nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
#           nn.BatchNorm2d(oup),
#       )
#     else:
#       self.conv = nn.Sequential(
#           # fused
#           nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),
#           nn.BatchNorm2d(hidden_dim),
#           SiLU(),
#           # pw-linear
#           nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
#           nn.BatchNorm2d(oup),
#       )

#   def forward(self, x):
#     if self.identity:
#       return x + self.conv(x)
#     else:
#       return self.conv(x)


# class EffNetV2(nn.Module):
#   def __init__(self, cfgs, num_classes=2, width_mult=1):
#     super(EffNetV2, self).__init__()
#     self.cfgs = cfgs

#     # 最初のレイヤーを作る。
#     input_channel = _make_divisible(24 * width_mult, 8)
#     layers = [conv_3x3_bn(3, input_channel, 2)]
#     # 反転したresidual blockを作る。
#     block = MBConv
#     for t, c, n, s, use_se in self.cfgs:
#       output_channel = _make_divisible(c * width_mult, 8)
#       for i in range(n):
#         layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))
#         input_channel = output_channel
#     self.features = nn.Sequential(*layers)
#     # ラストいくつかの層を作る。
#     output_channes = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792
#     self.conv = conv_1x1_bn(input_channel, output_channel)
#     self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
#     self.classifier = nn.Linear(output_channel, num_classes)

#     self._initialize_weight()

#   def forward(self, x):
#     # print(x.shape)
#     x = self.features(x)
#     # print(x.shape)
#     x = self.conv(x)
#     x = self.avgpool(x)
#     x = x.view(x.size(0), -1)
#     x = self.classifier(x)
#     return x

#   def _initialize_weight(self):
#     for m in self.modules():
#       if isinstance(m, nn.Conv2d):
#         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
#         m.weight.data.normal_(0, math.sqrt(2. / n))
#         if m.bias is not None:
#           m.bias.data.zero_()
#         elif isinstance(m, nn.BatchNorm2d):
#           m.weight.data.fill_(1)
#           m.bias.data.zero_()
#         elif isinstance(m, nn.Linear):
#           m.weight.data.normal_(0, 0.001)
#           m.bias.data.zero_()


# def effnetv2_s(**kwargs):
#   '''
#   Constructs a EfficientNetv2-s model
#   '''
#   cfgs = [
#           # t, c, n, s, SE
#           [1, 24, 2, 1, 0],
#           [4, 48, 4, 2, 0],
#           [4, 64, 4, 2, 0],
#           [4, 128, 6, 2, 1],
#           [6, 160, 9, 1, 1],
#           [6, 256, 15, 2, 1]
#   ]
#   return EffNetV2(cfgs, **kwargs)

# def effnetv2_m(**kwargs):
#     """
#     Constructs a EfficientNetV2-M model
#     """
#     cfgs = [
#         # t, c, n, s, SE
#         [1,  24,  3, 1, 0],
#         [4,  48,  5, 2, 0],
#         [4,  80,  5, 2, 0],
#         [4, 160,  7, 2, 1],
#         [6, 176, 14, 1, 1],
#         [6, 304, 18, 2, 1],
#         [6, 512,  5, 1, 1],
#     ]
#     return EffNetV2(cfgs, **kwargs)


# def effnetv2_l(**kwargs):
#     """
#     Constructs a EfficientNetV2-L model
#     """
#     cfgs = [
#         # t, c, n, s, SE
#         [1,  32,  4, 1, 0],
#         [4,  64,  7, 2, 0],
#         [4,  96,  7, 2, 0],
#         [4, 192, 10, 2, 1],
#         [6, 224, 19, 1, 1],
#         [6, 384, 25, 2, 1],
#         [6, 640,  7, 1, 1],
#     ]
#     return EffNetV2(cfgs, **kwargs)


# def effnetv2_xl(**kwargs):
#     """
#     Constructs a EfficientNetV2-XL model
#     """
#     cfgs = [
#         # t, c, n, s, SE
#         [1,  32,  4, 1, 0],
#         [4,  64,  8, 2, 0],
#         [4,  96,  8, 2, 0],
#         [4, 192, 16, 2, 1],
#         [6, 256, 24, 1, 1],
#         [6, 512, 32, 2, 1],
#         [6, 640,  8, 1, 1],
#     ]
#     return EffNetV2(cfgs, **kwargs)

# model = effnetv2_s().cuda()
# modelname = 'efficientnetv2'

import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
import os
from glob import glob
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import StepLR
import numpy as np
from datetime import datetime
import pandas as pd
import random 
from torchvision.datasets import ImageFolder
import re
import math
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from PIL import Image
from torch.optim.lr_scheduler import StepLR
from sklearn.metrics import roc_auc_score
from skimage.io import imread, imsave
import skimage
from PIL import ImageFile
from PIL import Image
torch.cuda.empty_cache()

from google.colab import drive
drive.mount('/content/drive')

a = glob('drive/MyDrive/Lung_Nodule_2/Image_Processed_2/Nodule/*')
b = glob('drive/MyDrive/Lung_Nodule_2/Image_Processed_2/Not_Nodule/*')

len(a), len(b)

## Rand Augment

from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageEnhance, ImageOps
import numpy as np
import random


class Rand_Augment():
    def __init__(self, Numbers=None, max_Magnitude=None):
        self.transforms = ['autocontrast', 'equalize', 'rotate', 'solarize', 'color', 'posterize',
                           'contrast', 'brightness', 'sharpness', 'shearX', 'shearY', 'translateX', 'translateY']
        if Numbers is None:
            self.Numbers = len(self.transforms) // 2
        else:
            self.Numbers = Numbers
        if max_Magnitude is None:
            self.max_Magnitude = 10
        else:
            self.max_Magnitude = max_Magnitude
        fillcolor = 128
        self.ranges = {
            # these  Magnitude   range , you  must test  it  yourself , see  what  will happen  after these  operation ,
            # it is no  need to obey  the value  in  autoaugment.py
            "shearX": np.linspace(0, 0.3, 10),
            "shearY": np.linspace(0, 0.3, 10),
            "translateX": np.linspace(0, 0.2, 10),
            "translateY": np.linspace(0, 0.2, 10),
            "rotate": np.linspace(0, 360, 10),
            "color": np.linspace(0.0, 0.9, 10),
            "posterize": np.round(np.linspace(8, 4, 10), 0).astype(np.int),
            "solarize": np.linspace(256, 231, 10),
            "contrast": np.linspace(0.0, 0.5, 10),
            "sharpness": np.linspace(0.0, 0.9, 10),
            "brightness": np.linspace(0.0, 0.3, 10),
            "autocontrast": [0] * 10,
            "equalize": [0] * 10,           
            "invert": [0] * 10
        }
        self.func = {
            "shearX": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),
                Image.BICUBIC, fill=fillcolor),
            "shearY": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),
                Image.BICUBIC, fill=fillcolor),
            "translateX": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),
                fill=fillcolor),
            "translateY": lambda img, magnitude: img.transform(
                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),
                fill=fillcolor),
            "rotate": lambda img, magnitude: self.rotate_with_fill(img, magnitude),
            # "rotate": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),
            "color": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),
            "posterize": lambda img, magnitude: ImageOps.posterize(img, magnitude),
            "solarize": lambda img, magnitude: ImageOps.solarize(img, magnitude),
            "contrast": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "sharpness": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "brightness": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(
                1 + magnitude * random.choice([-1, 1])),
            "autocontrast": lambda img, magnitude: ImageOps.autocontrast(img),
            "equalize": lambda img, magnitude: img,
            "invert": lambda img, magnitude: ImageOps.invert(img)
        }

    def rand_augment(self):
        """Generate a set of distortions.
             Args:
             N: Number of augmentation transformations to apply sequentially. N  is len(transforms)/2  will be best
             M: Max_Magnitude for all the transformations. should be  <= self.max_Magnitude """

        M = np.random.randint(0, self.max_Magnitude, self.Numbers)

        sampled_ops = np.random.choice(self.transforms, self.Numbers)
        return [(op, Magnitude) for (op, Magnitude) in zip(sampled_ops, M)]

    def __call__(self, image):
        operations = self.rand_augment()
        for (op_name, M) in operations:
            operation = self.func[op_name]
            mag = self.ranges[op_name][M]
            image = operation(image, mag)
        return image

    def rotate_with_fill(self, img, magnitude):
        #  I  don't know why  rotate  must change to RGBA , it is  copy  from Autoaugment - pytorch
        rot = img.convert("RGBA").rotate(magnitude)
        return Image.composite(rot, Image.new("RGBA", rot.size, (128,) * 4), rot).convert(img.mode)

    def test_single_operation(self, image, op_name, M=-1):
        '''
        :param image: image
        :param op_name: operation name in   self.transforms
        :param M: -1  stands  for the  max   Magnitude  in  there operation
        :return:
        '''
        operation = self.func[op_name]
        mag = self.ranges[op_name][M]
        image = operation(image, mag)
        return image


if __name__ == '__main__':
    # # this  is  for  call the whole fun
    # img_augment = Rand_Augment()
    # img_origal = Image.open(r'0a38b552372d.png')
    # img_final = img_augment(img_origal)
    # plt.imshow(img_final)
    # plt.show()
    # print('how to  call')

    # this  is for  a  single  fun  you  want to test
    img_augment = Rand_Augment()
    img_origal = Image.open('drive/MyDrive/instagram写真/line_2152454813573294.jpg')
    fig, ax = plt.subplots(5, 2, figsize=(10, 10))
    for i in range(0, 10):
        img_final = img_augment.test_single_operation(img_origal, 'rotate', M=i)
        ax[i//2, i%2].imshow(img_final)
    plt.show()
    print('how  to test')

########## trainデータセットの平均と標準偏差を使用。
normalize = transforms.Normalize(mean=[0.4305, 0.4305, 0.4305],
                                     std=[0.3505, 0.3505, 0.3505])

train_transformer = transforms.Compose([
    # transforms.Resize((600, 600)),
    transforms.Resize((260, 260)),
    # transforms.RandomResizedCrop((224),scale=(0.5,1.0)),
    # transforms.RandomHorizontalFlip(),
    # transforms.RandomRotation(90),
    # random brightness and random contrast 
    # brightness:明るさ, contrast:コントラスト, saturation:彩度, hue:色相
    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=3),
    transforms.ToTensor(),
    # normalize
])

val_transformer = transforms.Compose([
    # transforms.Resize((600, 600)),
    transforms.Resize((260, 260)),
    transforms.ToTensor(),
    # normalize
])

# '''Load LUNA dataset'''

# import h5py 
# import numpy as np
# import skimage
# import torch
# from torch.utils.data import DataLoader
# from torch.utils.data import TensorDataset
# f = h5py.File('all_patches.hdf5','r')
# f.keys()
# img = f['ct_slices'][:]  
# label = f['slice_class'][:] 
# f.close()
# print(np.shape(img))
# print('b',np.shape(label))
# skimage.io.imshow(img[120])
# print(label[120])
# batchsize=4

# class LungDataset(Dataset):
#     def __init__(self, img, label, transform=None):
#         self.img = img
#         self.label = label
#         self.transform = transform

#     def __len__(self):
#         return len(self.img)

#     def __getitem__(self, idx):
#         if torch.is_tensor(idx):
#             idx = idx.tolist()
        
#         image = PIL_image = Image.fromarray(self.img[idx]).convert('RGB')

#         if self.transform:
#             image = self.transform(image)
#         sample = {'img': image,
#                   'label': int(self.label[idx])}
#         return sample
    
# trainset = LungDataset(img, label, transform= val_transformer)
# valset = LungDataset(img, label, transform= val_transformer)
# train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)
# val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)
# modelname = 'medical_transfer'

batchsize=4
def read_txt(txt_path):
    with open(txt_path) as f:
        lines = f.readlines()
    txt_data = [line.strip() for line in lines]
    return txt_data

class CovidCTDataset(Dataset):
    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None, rand_augment=None):
        """
        Args:
            txt_path (string): Path to the txt file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        File structure:
        - root_dir
            - CT_COVID
                - img1.png
                - img2.png
                - ......
            - CT_NonCOVID
                - img1.png
                - img2.png
                - ......
        """
        self.root_dir = root_dir
        self.txt_path = [txt_COVID,txt_NonCOVID]
        # self.classes = ['CT_COVID', 'CT_NonCOVID']
        self.classes = ['Nodule', 'Not_Nodule']
        self.num_cls = len(self.classes)
        self.img_list = []
        for c in range(self.num_cls):

                                                                          # ここの c が正解ラベル！！！！
            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]

            self.img_list += cls_list
        self.rand_augment = rand_augment
        self.transform = transform

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_path = self.img_list[idx][0]
        # RGBで呼び出す必要はある？
        image = Image.open(img_path).convert('RGB')
        if self.rand_augment:
          image = self.rand_augment.__call__(image)

        if self.transform:
            image = self.transform(image)
        sample = {'img': image,
                  'label': int(self.img_list[idx][1])}
        return sample



    
if __name__ == '__main__':

    ### covid
    # trainset = CovidCTDataset(root_dir='drive/MyDrive/COVID-CT/Images-processed/',
    #                           txt_COVID='drive/MyDrive/COVID-CT/Data-split/COVID/trainCT_COVID.txt',
    #                           txt_NonCOVID='drive/MyDrive/COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt',
    #                           transform=train_transformer,
    #                           rand_augment=Rand_Augment(Numbers=1, max_Magnitude=5))
    # valset = CovidCTDataset(root_dir='drive/MyDrive/COVID-CT/Images-processed/',
    #                           txt_COVID='drive/MyDrive/COVID-CT/Data-split/COVID/valCT_COVID.txt',
    #                           txt_NonCOVID='drive/MyDrive/COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt',
    #                           transform=val_transformer)
    # testset = CovidCTDataset(root_dir='drive/MyDrive/COVID-CT/Images-processed/',
    #                           txt_COVID='drive/MyDrive/COVID-CT/Data-split/COVID/test2CT_COVID.txt',
    #                           txt_NonCOVID='drive/MyDrive/COVID-CT/Data-split/NonCOVID/test2CT_NonCOVID.txt',
    #                           transform=val_transformer)

    ### cancer drive/MyDrive/Lung_Nodule_2/Data_split/
    trainset = CovidCTDataset(root_dir='drive/MyDrive/Lung_Nodule_2/Image_Processed_2/',
                              txt_COVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/train_nodule.txt',
                              txt_NonCOVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/train_not_nodule.txt',
                              transform=train_transformer,
                              rand_augment=Rand_Augment(Numbers=1, max_Magnitude=5))
    valset = CovidCTDataset(root_dir='drive/MyDrive/Lung_Nodule_2/Image_Processed_2/',
                              txt_COVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/val_nodule.txt',
                              txt_NonCOVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/val_not_nodule.txt',
                              transform=val_transformer)
    testset = CovidCTDataset(root_dir='drive/MyDrive/Lung_Nodule_2/Image_Processed_2/',
                              txt_COVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/test_nodule.txt',
                              txt_NonCOVID='drive/MyDrive/Lung_Nodule_2/Data_Split_2/test_not_nodule.txt',
                              transform=val_transformer)
    
    print('{}'.format(trainset.__len__()))
    print('{}'.format(valset.__len__()))
    print('{}'.format(testset.__len__()))

    train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)
    val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)
    test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)

batch_data = next(iter(train_loader))
data = batch_data['img']
label = batch_data['label']

"""# バッチサイズが4だから、shapeがこんな値
plt.imshow(trainset[0]['img'][0,:,:])
print(trainset[0]['label'])
"""

fig, ax = plt.subplots(1,4,figsize=(20,40))
ax[0].imshow(data[0,0,:,:], cmap='gray')
ax[0].set_xlabel('{}'.format(label[0]))

ax[1].imshow(data[1,0,:,:], cmap='gray')
ax[1].set_xlabel('{}'.format(label[1]))

ax[2].imshow(data[2,0,:,:], cmap='gray')
ax[2].set_xlabel('{}'.format(label[2]))

ax[3].imshow(data[3,0,:,:], cmap='gray')
ax[3].set_xlabel('{}'.format(label[3]))

lam = np.random.beta(alpha, alpha)
lam

0.2 * lam + (1 - lam)

# mixup

def mixup_data(x, y, alpha=1.0, use_cuda=True):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
        # print('lam',lam)
    else:
        lam = 1

    batch_size = x.size()[0]
    if use_cuda:
        index = torch.randperm(batch_size).cuda() # torch.randperm(4) -> 例)tensor[2, 1, 3, 4]
    else:
        index = torch.randperm(batch_size)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    # print(pred)
    # print(y_a)
    # print('criterion',criterion(pred, y_a))
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

#training process 

alpha = 1
# alpah = None

## alpha is None if mixup is not used
# alpha_name = f'{alpha}'
device = 'cuda'

def train(optimizer, epoch):
    # print(epoch)
    
    model.train()
    # writer = SummaryWriter()

    train_loss = 0
    train_correct = 0
    # if tfboard_dir:
    #   from tensorboardX import SummaryWriter
    #   tblogger = SummaryWriter(tfboard_dir)

    
    for batch_index, batch_samples in enumerate(train_loader):
        
        data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)   
        
        #mixup
        data, targets_a, targets_b, lam = mixup_data(data, target, alpha, use_cuda=True)
        
        
        optimizer.zero_grad()
        output = model(data)
        criteria = nn.CrossEntropyLoss()
        # loss = criteria(output, target.long())

        #mixup loss
        loss = mixup_criterion(criteria, output, targets_a, targets_b, lam)

        train_loss += criteria(output, targets_b.long())
        # train_loss += loss
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        pred = output.argmax(dim=1, keepdim=True)
        train_correct += pred.eq(target.long().view_as(pred)).sum().item()
    
        if batch_index % bs == 0:
          print('Train Epoch: {} [{}/{} ({:.0f}%)]\tTrain Loss: {:.6f}'.format(
              epoch, batch_index, len(train_loader),
              100.0 * batch_index / len(train_loader), loss.item()/ bs))
          
    return train_loss.item() / (len(train_loader)* 4)  # 4はバッチサイズ

#val process

def val(epoch):
    
    model.eval()
    test_loss = 0
    correct = 0
    results = []
    
    TP = 0
    TN = 0
    FN = 0
    FP = 0
    
    
    criteria = nn.CrossEntropyLoss()
    # モデルをアップデートしない！
    with torch.no_grad():
        tpr_list = []
        fpr_list = []
        
        predlist=[]
        scorelist=[]
        targetlist=[]
        # Predict
        for batch_index, batch_samples in enumerate(val_loader):
            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)
            
#             data = data[:, 0, :, :]
#             data = data[:, None, :, :]
            output = model(data)
            
            test_loss += criteria(output, target.long())
            score = F.softmax(output, dim=1)
            pred = output.argmax(dim=1, keepdim=True)
#             print('target',target.long()[:, 2].view_as(pred))
            correct += pred.eq(target.long().view_as(pred)).sum().item()
            
#             print(output[:,1].cpu().numpy())
#             print((output[:,1]+output[:,0]).cpu().numpy())
#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())
            targetcpu=target.long().cpu().numpy()
            predlist=np.append(predlist, pred.cpu().numpy())
            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])
            targetlist=np.append(targetlist,targetcpu)
           
    return targetlist, scorelist, predlist, test_loss.item()/(len(val_loader)* 4)  #4はバッチサイズ
    
    # Write to tensorboard
#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)

#test process 

def test(epoch):
    
    model.eval()
    test_loss = 0
    correct = 0
    results = []
    
    TP = 0
    TN = 0
    FN = 0
    FP = 0
    
    
    criteria = nn.CrossEntropyLoss()
    # モデルをアップデートしない。
    with torch.no_grad():
        tpr_list = []
        fpr_list = []
        
        predlist=[]
        scorelist=[]
        targetlist=[]
        # Predict
        for batch_index, batch_samples in enumerate(test_loader):
            data, target = batch_samples['img'].to(device), batch_samples['label'].to(device)
#             data = data[:, 0, :, :]
#             data = data[:, None, :, :]
#             print(target)
            output = model(data)
            
            test_loss += criteria(output, target.long())
            score = F.softmax(output, dim=1)
            pred = output.argmax(dim=1, keepdim=True)
#             print('target',target.long()[:, 2].view_as(pred))
            correct += pred.eq(target.long().view_as(pred)).sum().item()
#             TP += ((pred == 1) & (target.long()[:, 2].view_as(pred).data == 1)).cpu().sum()
#             TN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()
# #             # FN    predict 0 label 1
#             FN += ((pred == 0) & (target.long()[:, 2].view_as(pred) == 1)).cpu().sum()
# #             # FP    predict 1 label 0
#             FP += ((pred == 1) & (target.long()[:, 2].view_as(pred) == 0)).cpu().sum()
#             print(TP,TN,FN,FP)
            
            
#             print(output[:,1].cpu().numpy())
#             print((output[:,1]+output[:,0]).cpu().numpy())
#             predcpu=(output[:,1].cpu().numpy())/((output[:,1]+output[:,0]).cpu().numpy())
            targetcpu=target.long().cpu().numpy()
            predlist=np.append(predlist, pred.cpu().numpy())
            scorelist=np.append(scorelist, score.cpu().numpy()[:,1])
            targetlist=np.append(targetlist,targetcpu)
    return targetlist, scorelist, predlist
    
    # Write to tensorboard
#     writer.add_scalar('Test Accuracy', 100.0 * correct / len(test_loader.dataset), epoch)

# '''DenseNet121 pretrained model from xrv'''

# class DenseNetModel(nn.Module):

#     def __init__(self):
#         """
#         Pass in parsed HyperOptArgumentParser to the model
#         :param hparams:
#         """
#         super(DenseNetModel, self).__init__()

#         self.dense_net = xrv.models.DenseNet(num_classes=2)
#         self.criterion = nn.CrossEntropyLoss()

#     def forward(self, x):
#         logits = self.dense_net(x)
#         return logits
  
# model = DenseNetModel().cuda()
# modelname = 'DenseNet_medical'
# # print(model)

'''ResNet18 pretrained'''
import torchvision.models as models
model = models.resnet18(pretrained=True).cuda()
modelname = 'ResNet18'

# '''Dense121 pretrained'''
import torchvision.models as models
model = models.densenet121(pretrained=True)
modelname = 'Dense121'
# pretrained_net = torch.load('')
# model.load_state_dict(pretrained_net)

### Dense169
import torchvision.models as models
model = models.densenet169(pretrained=True).cuda()
modelname = 'Dense169'

# """load MoCo pretrained model"""
# checkpoint = torch.load('new_data/save_model_dense/checkpoint_luna_covid_moco.pth.tar')
# # # # print(checkpoint.keys())
# # # # print(checkpoint['arch'])

# state_dict = checkpoint['state_dict']
# for key in list(state_dict.keys()):
#     if 'module.encoder_q' in key:
# #         print(key[17:])
#         new_key = key[17:]
#         state_dict[new_key] = state_dict[key]
#     del state_dict[key]
# for key in list(state_dict.keys()):
#     if  key == 'classifier.0.weight':
#         new_key = 'classifier.weight'
#         state_dict[new_key] = state_dict[key]
#         del state_dict[key]
#     if  key == 'classifier.0.bias':
#         new_key = 'classifier.bias'
#         state_dict[new_key] = state_dict[key]
#         del state_dict[key]
#     if  key == 'classifier.2.weight' or key == 'classifier.2.bias':
#         del state_dict[key]
# state_dict['classifier.weight'] = state_dict['classifier.weight'][:1000,:]
# state_dict['classifier.bias'] = state_dict['classifier.bias'][:1000]
# model.load_state_dict(checkpoint['state_dict'])

# # # print(model)

# """Load Self-Trans model"""
# """Change names and locations to the Self-Trans.pt"""


# model = models.densenet169(pretrained=True).cuda()
# # pretrained_net = torch.load('model_backup/Dense169.pt')
# # pretrained_net = torch.load('model_backup/mixup/Dense169_0.6.pt')
# pretrained_net = torch.load('model_backup/medical_transfer/medical_transfer_None_LUNA_moco_covid_moco.pt')


# model.load_state_dict(pretrained_net)

# modelname = 'Dense169_ssl_luna_moco'

'''ResNet50 pretrained'''

import torchvision.models as models
model = models.resnet50(pretrained=True).cuda()

# checkpoint = torch.load('new_data/save_model/checkpoint.pth.tar')
# # print(checkpoint.keys())
# # print(checkpoint['arch'])

# state_dict = checkpoint['state_dict']
# for key in list(state_dict.keys()):
#     if 'module.encoder_q' in key:
#         print(key[17:])
#         new_key = key[17:]
#         state_dict[new_key] = state_dict[key]
#     del state_dict[key]
# for key in list(state_dict.keys()):
#     if  key == 'fc.0.weight':
#         new_key = 'fc.weight'
#         state_dict[new_key] = state_dict[key]
#         del state_dict[key]
#     if  key == 'fc.0.bias':
#         new_key = 'fc.bias'
#         state_dict[new_key] = state_dict[key]
#         del state_dict[key]
#     if  key == 'fc.2.weight' or key == 'fc.2.bias':
#         del state_dict[key]
# state_dict['fc.weight'] = state_dict['fc.weight'][:1000,:]
# state_dict['fc.bias'] = state_dict['fc.bias'][:1000]
# # print(state_dict.keys())

# # print(state_dict)
# # pattern = re.compile(
# #         r'^(.*denselayer\d+\.(?:norm|relu|conv))\.((?:[12])\.(?:weight|bias|running_mean|running_var))$')
# #     for key in list(state_dict.keys()):
# #         match = pattern.match(key)
# #         new_key = match.group(1) + match.group(2) if match else key
# #         new_key = new_key[7:] if remove_data_parallel else new_key
# #         new_key = new_key[7:]
# #         state_dict[new_key] = state_dict[key]
# #         del state_dict[key]
    
# # model.load_state_dict(checkpoint['state_dict'])
    
# # # # modelname = 'ResNet50'
modelname = 'ResNet50_ssl'

'''VGGNet pretrained'''
import torchvision.models as models
model = models.vgg16(pretrained=True)
model = model.cuda()
modelname = 'vgg16'

# '''efficientNet pretrained b0'''

! pip install efficientnet_pytorch 

from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)
model = model.cuda()
modelname = 'efficientNet-b0'


# model = EfficientNet.from_name('efficientnet-b1').cuda()
# modelname = 'efficientNet_random'

# '''efficientNet pretrained b2'''

! pip install efficientnet_pytorch 

from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b2', num_classes=2)
model = model.cuda()
modelname = 'efficientNet-b2'


# model = EfficientNet.from_name('efficientnet-b1').cuda()
# modelname = 'efficientNet_random'

# '''efficientNet pretrained b7'''

! pip install efficientnet_pytorch 

from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b7', num_classes=2)
model = model.cuda()
modelname = 'efficientNet-b7'


# model = EfficientNet.from_name('efficientnet-b1').cuda()
# modelname = 'efficientNet_random'

# 上記で選んだモデルたちは1000クラス分類仕様になっているので、最終層の出力を2クラスにする。
import torchvision.models
class customize_model(nn.Module):
  def __init__(self):
    super().__init__()
    # self.models = torchvision.models.densenet169(pretrained=True)
    self.models = EfficientNet.from_pretrained('efficientnet-b0')
    self.f = nn.Sequential(
        nn.Linear(in_features=np.int(1000), out_features=2, bias=True),
        nn.Softmax(dim=1),
        )

  def forward(self, x):
    x = self.models(x)
    out = self.f(x)
    return out


model = customize_model().cuda()
modelname = 'Densenet121(pre)'

# Commented out IPython magic to ensure Python compatibility.
! pip install tensorboardX
# colabでtensorboardを使うには、以下の文やマジックコマンドが必要。
from __future__ import absolute_import, division, print_function, unicode_literals

try:
#   %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

# %load_ext tensorboard



# train

loss_train = []
loss_val = []

bs = batchsize
total_epoch = 20


votenum = 100
checkpoint_interval = 20
checkpoint_dir = 'drive/MyDrive/Checkpoint_CT'

import warnings
warnings.filterwarnings('ignore')

logs_dir = 'drive/MyDrive/CT-Logs'
# from tensorboardX import SummaryWriter
# writer = SummaryWriter(logs_dir)

#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)
optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)
                                             
scheduler = StepLR(optimizer, step_size=1)


# checkpointファイルがあれば、パスを記載。
checkpoint = None
checkpoint = 'drive/MyDrive/Checkpoint_CT/snapshot180.ckpt'

iter_state = 1
if checkpoint:
  print('loading pytorch ckpt...', checkpoint)
  state = torch.load(checkpoint)

  if 'model_state_dict' in state.keys():
    model.load_state_dict(state['model_state_dict'])
  else:
    model.load_state_dict(state)

if checkpoint:
  if 'optimizer_state_dict' in state.keys():
    optimizer.load_state_dict(state['optimizer_state_dict'])
    iter_state = state['epoch'] + 1


r_list = []
p_list = []
acc_list = []
AUC_list = []
# TP = 0
# TN = 0
# FN = 0
# FP = 0
vote_pred = np.zeros(valset.__len__())
vote_score = np.zeros(valset.__len__())


for epoch in range(iter_state, iter_state + total_epoch if iter_state == 1 else iter_state + total_epoch + 1):
    
    train_lo = train(optimizer, epoch)
    # writer.add_scalar('train_loss', train_lo, epoch)

    targetlist, scorelist, predlist, val_lo = val(epoch)
    # writer.add_scalar('val_loss', val_lo, epoch)
    # print('target',targetlist)
    # print('score',scorelist)
    # print('predict',predlist)
    vote_pred = vote_pred + predlist 
    vote_score = vote_score + scorelist 

    loss_train.append(train_lo)
    loss_val.append(val_lo)
    

    if epoch % votenum == 0:
        
        # major vote
        vote_pred[vote_pred <= (votenum/2)] = 0
        vote_pred[vote_pred > (votenum/2)] = 1
        vote_score = vote_score/votenum
        
        print('vote_pred', vote_pred)
        print('targetlist', targetlist)
        TP = ((vote_pred == 1) & (targetlist == 1)).sum()
        TN = ((vote_pred == 0) & (targetlist == 0)).sum()
        FN = ((vote_pred == 0) & (targetlist == 1)).sum()
        FP = ((vote_pred == 1) & (targetlist == 0)).sum()
        
        
        print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)
        print('TP+FP',TP+FP)
        p = TP / (TP + FP)
        print('precision',p)
        p = TP / (TP + FP)
        r = TP / (TP + FN)
        print('recall',r)
        F1 = 2 * r * p / (r + p)
        acc = (TP + TN) / (TP + TN + FP + FN)
        print('F1',F1)
        print('acc',acc)
        AUC = roc_auc_score(targetlist, vote_score)
        print('AUCp', roc_auc_score(targetlist, vote_pred))
        print('AUC', AUC)
        
        
        
#         if epoch == total_epoch:
        # torch.s_ave(model.state_dict(), "model_backup/medical_transfer/{}_{}_covid_moco_covid.pt".format(modelname,alpha_name))  
        
        vote_pred = np.zeros(valset.__len__())
        vote_score = np.zeros(valset.__len__())
        print('\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\
average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(
        epoch, r, p, F1, acc, AUC))
        print(modelname)

    if epoch > 0 and (epoch % checkpoint_interval == 0):
      torch.save({'epoch': epoch,
                  'model_state_dict': model.state_dict(),
                  'optimizer_state_dict': optimizer.state_dict(),
                  },
                 os.path.join(checkpoint_dir, 'snapshot'+str(epoch)+'.ckpt'))
#         f = open('model_result/medical_transfer/{}_{}.txt'.format(modelname,alpha_name), 'a+')
#         f.write('\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\
# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(
#         epoch, r, p, F1, acc, AUC))
#         f.close()

# writer.close()

import seaborn as sns
import pandas as pd

df = pd.DataFrame(loss_train)
sns.regplot(x=list(range(0,101)), y=loss_val, order=3)

range(100)

fig, ax = plt.subplots()
ax.plot(loss_train)
ax.plot(loss_val)
ax.set_xlabel('epoch')
ax.set_ylabel('train_loss')

# ax[1].plot(loss_val)
# ax[1].set_xlabel('epoch')
# ax[1].set_ylabel('val_loss')

fig.tight_layout()

fig.savefig('drive/MyDrive/Checkpoint_CT/')

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=drive/MyDrive/CT-Logs

state = torch.load('drive/MyDrive/Checkpoint_CT/snapshot200.ckpt')
model.load_state_dict(state['model_state_dict'])

# test
bs = 10
import warnings
warnings.filterwarnings('ignore')

epoch = 1
r_list = []
p_list = []
acc_list = []
AUC_list = []
# TP = 0
# TN = 0
# FN = 0
# FP = 0
vote_pred = np.zeros(testset.__len__())
vote_score = np.zeros(testset.__len__())


targetlist, scorelist, predlist = test(epoch)
print('target',targetlist)
print('score',scorelist)
print('predict',predlist)
vote_pred = vote_pred + predlist 
vote_score = vote_score + scorelist 

TP = ((predlist == 1) & (targetlist == 1)).sum()

TN = ((predlist == 0) & (targetlist == 0)).sum()
FN = ((predlist == 0) & (targetlist == 1)).sum()
FP = ((predlist == 1) & (targetlist == 0)).sum()

print('TP=',TP,'TN=',TN,'FN=',FN,'FP=',FP)
print('TP+FP',TP+FP)
p = TP / (TP + FP)
print('precision',p)
p = TP / (TP + FP)
r = TP / (TP + FN)
print('recall',r)
F1 = 2 * r * p / (r + p)
acc = (TP + TN) / (TP + TN + FP + FN)
print('F1',F1)
print('acc',acc)
AUC = roc_auc_score(targetlist, vote_score)
print('AUC', AUC)

# f = open(f'model_result/medical_transfer/test_{modelname}_{alpha_name}_LUNA_moco_CT_moco.txt', 'a+')
# f.write('\n The epoch is {}, average recall: {:.4f}, average precision: {:.4f},\
# average F1: {:.4f}, average accuracy: {:.4f}, average AUC: {:.4f}'.format(
# epoch, r, p, F1, acc, AUC))
# f.close()
# torch.save(model.state_dict(), "model_backup/medical_transfer/{}_{}_covid_moco_covid.pt".format(modelname,alpha_name))

model]

